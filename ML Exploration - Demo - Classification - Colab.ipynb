{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Exploration Notebook\n",
    "\n",
    "This notebook can be used to explore the data of an underlying problem and see if the data is suited for predictive analysis. Several classifiers will be compared on predictive performance metrics such as accuracy, precision, recall and area under curve, such that the user can get a head start in solving the problem or managing expectations.\n",
    "\n",
    "The notebook is structured in the following way:\n",
    "\n",
    "    1. Set project path\n",
    "    2. Read in (raw) Data Set\n",
    "    3. Basic Data Information\n",
    "    4. Set Dependent & Independent Variables \n",
    "    5. Set Parameter Values\n",
    "    6. Generate Predictions\n",
    "    7. Compare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set project path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make use of the files stored on Github via Colab, we first have to clone the folder on Github to the current Colab-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the entire repo. Remove the # from the next two lines.\n",
    "!git clone -s git://github.com/PippleNL/Pipple-Lecture-8-ML-prediction.git ML_copy\n",
    "%cd ML_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use package os to set the correct project_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in (raw) Data Set\n",
    "\n",
    "The (raw) data set of the underlying problem is read from a comma seperated file (.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = os.path.join(project_path, 'data', 'Beer_data.csv')\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Data Information\n",
    "\n",
    "Below you can find some basic information of the data set. It lists the first couple of rows, a summary of the dataframe including the dtype (data-type) and number of non-null values per column and the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to change a column with dtype 'object' to 'numeric', you can use the following function. This is only possible if the column actually contains numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.data_preparation import column2num\n",
    "columns2num = []\n",
    "\n",
    "if len(columns2num) > 0:\n",
    "    data = column2num(data, columns2num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of NaN values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll state the number of unique values per column. If a column only has one value, it will not have any impact on a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique Values for Each Feature: \\n')\n",
    "for i in data.columns:\n",
    "    print(i, ':', data[i].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the correlation matrix to get an idea of relations between the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(9,9)) \n",
    "corr_mat = round(data.corr(method='pearson'), 2)\n",
    "sns.heatmap(corr_mat, vmin=-1, vmax=1, center=0, annot=True, cmap=sns.diverging_palette(20, 220, n=200), square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification problem the dependent variable is often no numeric variable and therefore it's correlation with the other variables is not given. To get some feeling about the dependencies between columns in terms of correlation, we can first transform the categorical variables into numerical variables and then again create the correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "label_encoder_1 = LabelEncoder()\n",
    "\n",
    "object_columns = data_copy.dtypes == np.object  #get array with True/False indicating for each column if it is a object\n",
    "for object_column in data_copy.columns[object_columns]:\n",
    "    label_transformed = label_encoder_1.fit_transform(data_copy[object_column]) # fit and tranform data per column\n",
    "    data_copy[object_column] = label_transformed # Replace categorical values with transformed numerical values\n",
    "\n",
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,9)) \n",
    "corr_mat = round(data_copy.corr(method='pearson'), 2)\n",
    "sns.heatmap(corr_mat, vmin=-1, vmax=1, center=0, annot=True, cmap=sns.diverging_palette(20, 220, n=200), square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set Dependent & Independent Variables\n",
    "\n",
    "Specify below in string which variable (i.e. column) will be used as dependent variable. This variable will be set as y (i.e. label) and will ultimately be modeled. Also, specify a list of other (independent) variables in string that are used to explain the dependent variable. If empty, all other variables will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent = 'Score'  # fill in your dependent variable here.\n",
    "independent = ['Calories', 'RandomGenerator', 'Acid', 'Belgian']  # fill the list of independent variables here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Set Parameter Values\n",
    "\n",
    "Specify below the parameter values used while comparing models. If kept commented, the default value will be used. If uncommented, added these parameters to the function main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_strategy = 0.  # either a float or 'drop' (default), 'mean', 'median', 'most_frequent'\n",
    "labelenc_x = ['Belgian']  # fill the list of independent variables for label encoding here..., if empty then []\n",
    "onehotenc_x = []  # fill the list of independent variables for one hot encoding, if empty then []\n",
    "labelenc_y = True  # boolean specifying if label encoding for y variable is necessary\n",
    "feature_scaling = 'auto'  # None, 'standardisation', 'minmax' or 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate Predictions\n",
    "\n",
    "Predictions are generated for several models using the function 'main_classificaion' from the Python script 'compare_models'. Note that if not specified differently, all default parameter values are used. More information can be retrieved by running 'main_classification()'. The function gives two lists; predictions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functions.compare_models import main_classification\n",
    "predictions, classes = main_classification(data, dependent, independent, impute_strategy=impute_strategy, labelenc_x=labelenc_x, onehotenc_x=onehotenc_x, feature_scaling_method=feature_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some additional information\n",
    "\n",
    "- True Positives; Number of correctly identified 'Positive' Values; model says that is 'Positive' and in reality it is 'Positive' \n",
    "- False Positives; Model says that it is 'Positive', but in reality it is not 'Positive'\n",
    "- True Negatives; Number of correctly identified 'Negative' Values; model says 'Negative' and in reality it is\n",
    "- False Negatives; Model says that it is 'Negative', but in reality it is not 'Negative'\n",
    "\n",
    "#### Evaluation metrics for classification\n",
    "- Accuracy = ratio of correctly predicted classes -> (True Positive + True Negative)/(all observations) \n",
    "- Precision = Given that model predicts a class, how many are in reality that class -> (True Positive)/(True Positive + False Positive)\n",
    "- Recall = Given that in reality it is a class, how many are predicted by the model -> (True Positive) / (True Positive + False Negative)\n",
    "https://en.wikipedia.org/wiki/Sensitivity_and_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compare Models\n",
    "\n",
    "Models are compared based on predictive performance metrics that are calculated and sorted by the (own-developed) function 'sort_compute_metrics_clf' in the Python script 'compare_models'. More information on the function can be retrieved using sort_compute_metrics_clf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.compare_models import sort_compute_metrics_clf\n",
    "multi_class = True if len(classes) > 2 else False\n",
    "header, scores = sort_compute_metrics_clf(predictions, multi_class=multi_class)\n",
    "pd.DataFrame(scores, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "yticks_workaround = [i for i in classes] #Workaround for error in package plt version 3.1.1.\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "    cm = confusion_matrix(predictions[i][2], predictions[i][1])\n",
    "    plt.figure(figsize = (6,6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    #df_cm = pd.DataFrame(cm, index = [i for i in classes], columns = [i for i in classes])\n",
    "    #sns.heatmap(df_cm, annot=True, ax = ax, fmt='.3g')\n",
    "\n",
    "    # Workaround for error in package plt version 3.1.1\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    for row in range(0,len(classes)):\n",
    "        for column in range(0,len(classes)):\n",
    "            value = cm[column,row]\n",
    "            ax.text(row, column, str(value), va='center', ha='center')\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_xticks(list(range(0,len(classes))))\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.set_yticklabels([''] + yticks_workaround)    # Workaround for plt version 3.1.1.\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix - '+ predictions[i][0])"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
